# ü§ñ AI Provider Configuration Guide (English)

> üåê **[README Bahasa Indonesia](README.md)** | **[README English](README_EN.md)** | **[AI Provider Guide (ID)](AI_PROVIDER_GUIDE.md)**

## ‚úÖ Current Status
- **Active Provider**: Configurable (depends on .env.local settings)
- **Validation**: ‚úÖ Working properly
- **Application**: ‚úÖ Running successfully with proper configuration

## üîß Validation System

The application automatically validates AI provider configuration on startup:

### ‚úÖ Valid Configuration
```bash
# Only one provider with both API key/URI and model configured
OLLAMA_URI=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b
```
**Result**: ‚úÖ `Using Ollama (ollama) with model: qwen2.5:7b`

### ‚ùå No Provider Configured
```bash
# All providers commented or empty
# GROQ_API=
# GROQ_MODEL=
# OPENROUTER_API=
# OPENROUTER_MODEL=
```
**Result**: ‚ùå Error asking to configure one provider with model

### ‚ùå Missing Model Configuration
```bash
# Provider configured but model missing
GROQ_API=gsk_your_key
# GROQ_MODEL=  <- Missing!
```
**Result**: ‚ùå Error asking to set model for the provider

### ‚ùå Multiple Providers Configured
```bash
# Multiple providers active
GROQ_API=your_key
GROQ_MODEL=llama-3.1-70b-versatile
OLLAMA_URI=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b
```
**Result**: ‚ùå Error asking to configure only one provider

## üöÄ Supported Providers

### 1. Groq (Recommended)
```bash
GROQ_API=gsk_...
GROQ_MODEL=llama-3.1-70b-versatile
```
- **Model**: Configurable (llama-3.1-70b-versatile, etc.)
- **Speed**: ‚ö° Very fast inference
- **Cost**: üí∞ Pay per token
- **Setup**: Register at [console.groq.com](https://console.groq.com)

### 2. OpenRouter
```bash
OPENROUTER_API=sk-or-v1-...
OPENROUTER_MODEL=google/gemini-2.5-flash
```
- **Model**: Configurable (google/gemini-2.5-flash, etc.)
- **Speed**: ‚ö° Fast
- **Cost**: üí∞ Pay per token
- **Setup**: Register at [openrouter.ai](https://openrouter.ai)

### 3. Ollama (Local)
```bash
OLLAMA_URI=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b
```
- **Model**: Configurable (qwen2.5:7b, llama3.1, etc.)
- **Speed**: üîÑ Depends on hardware
- **Cost**: üÜì Free (local)
- **Setup**: Install [Ollama](https://ollama.ai) + `ollama pull qwen2.5:7b`

### 4. LM Studio (Local)
```bash
LM_STUDIO_URI=http://localhost:1234/v1
LM_STUDIO_MODEL=local-model
```
- **Model**: üîß Configurable (depends on loaded model)
- **Speed**: üîÑ Depends on hardware
- **Cost**: üÜì Free (local)
- **Setup**: Install [LM Studio](https://lmstudio.ai) + start server

## üß™ Testing Configuration

To test your configuration, check the console output when starting the development server:
```bash
pnpm dev
```

Look for validation messages like:
```
üîß Validating AI configuration...
‚úÖ Using [Provider] ([type]) with model: [model_name]
```

## üìù Environment Variables Template

```bash
# AI Providers - CONFIGURE ONLY ONE AT A TIME!
# Both API key/URI and MODEL are required for each provider

# Option 1: Groq (Recommended)
# GROQ_API=gsk_your_groq_api_key
# GROQ_MODEL=llama-3.1-70b-versatile

# Option 2: OpenRouter
# OPENROUTER_API=sk-or-v1-your_openrouter_api_key
# OPENROUTER_MODEL=google/gemini-2.5-flash

# Option 3: Ollama (Local)
# OLLAMA_URI=http://localhost:11434
# OLLAMA_MODEL=qwen2.5:7b

# Option 4: LM Studio (Local)
# LM_STUDIO_URI=http://localhost:1234/v1
# LM_STUDIO_MODEL=local-model
```

## üîÑ Switching Providers

1. Comment out current provider (both variables)
2. Uncomment desired provider
3. Add your API key/URI and model name
4. Restart development server

Example switch from Ollama to Groq:
```bash
# Option 1: Groq
GROQ_API=gsk_your_groq_api_key
GROQ_MODEL=llama-3.1-70b-versatile

# Option 3: Ollama 
# OLLAMA_URI=http://localhost:11434
# OLLAMA_MODEL=qwen2.5:7b
```

## üõ°Ô∏è Error Handling

The application includes comprehensive error handling:
- ‚úÖ Startup validation with clear error messages
- ‚úÖ Runtime error handling for API failures
- ‚úÖ Fallback responses for invalid AI responses
- ‚úÖ Rate limiting protection with Arcjet

## üîç Common Issues

### Issue: "No AI provider configured properly!"
**Solution**: Make sure both API key/URI and MODEL are set for one provider.

### Issue: "Multiple AI providers configured"
**Solution**: Comment out all providers except one.

### Issue: "Model not configured for [Provider]"
**Solution**: Set the MODEL environment variable for your chosen provider.

### Issue: AI responses are inconsistent
**Solution**: Check if your chosen model is appropriate for chat-based interactions.

## üìö Model Recommendations

### For Chat/Conversation:
- **Groq**: `llama-3.1-70b-versatile`, `llama-3.1-8b-instant`
- **OpenRouter**: `google/gemini-2.5-flash`, `anthropic/claude-3.5-sonnet`
- **Ollama**: `qwen2.5:7b`, `llama3.1:8b`

### For JSON Output:
- **Groq**: `llama-3.1-70b-versatile` (best JSON compliance)
- **OpenRouter**: `google/gemini-2.5-flash` (good structured output)
- **Ollama**: `qwen2.5:7b` (decent JSON support)

## üéØ Performance Tips

1. **Groq**: Fastest inference, good for production
2. **OpenRouter**: Good balance of speed and model variety
3. **Ollama**: Best for privacy, requires good hardware
4. **LM Studio**: Easy local setup, GUI management

Choose based on your priorities: speed, cost, privacy, or ease of use.
